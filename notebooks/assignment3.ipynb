{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e7435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3: DQN for Pong - Expert Notebook\n",
    "\n",
    "# 1. Imports and Seed Setting\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from assignment3_utils import img_crop, downsample, to_grayscale, normalize_grayscale, process_frame, transform_reward\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a220b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f247adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state shape: (4, 84, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "# 2. Environment Setup\n",
    "image_shape = (84, 80)\n",
    "\n",
    "def make_env():\n",
    "    try:\n",
    "        env = gym.make('ALE/Pong-v5')\n",
    "    except Exception:\n",
    "        env = gym.make('Pong-v4')\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "# 3. Frame Preprocessing Integration\n",
    "def preprocess_frame(img: np.ndarray, image_shape: tuple) -> np.ndarray:\n",
    "    img = img_crop(img)\n",
    "    img = downsample(img)\n",
    "    img = to_grayscale(img)\n",
    "    img = normalize_grayscale(img)\n",
    "    return np.expand_dims(img.reshape(image_shape[0], image_shape[1], 1), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def get_initial_state(env, image_shape):\n",
    "    obs, info = env.reset()\n",
    "    processed = preprocess_frame(obs, image_shape).squeeze(0)\n",
    "    return np.stack([processed] * 4, axis=0)  # shape (4, 84, 80)\n",
    "\n",
    "print(f\"Initial state shape: {get_initial_state(env, image_shape).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "337e9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DQN Network\n",
    "class DQNCNN(nn.Module):\n",
    "    def __init__(self, input_channels, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n",
    "        )\n",
    "        conv_w = self._conv2d_size_out(self._conv2d_size_out(self._conv2d_size_out(image_shape[1],8,4),4,2),3,1)\n",
    "        conv_h = self._conv2d_size_out(self._conv2d_size_out(self._conv2d_size_out(image_shape[0],8,4),4,2),3,1)\n",
    "        linear_input_size = conv_w * conv_h * 64\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    def _conv2d_size_out(self, size, kernel_size, stride, padding=0):\n",
    "        return (size - kernel_size + 2*padding) // stride + 1\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7fa63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example test\n",
    "input_channels = 4  # 4 stacked frames\n",
    "n_actions = env.action_space.n  # number of actions in Pong\n",
    "model = DQNCNN(input_channels, n_actions)\n",
    "\n",
    "# Dummy input: batch of 1, shape (1, 4, 84, 80)\n",
    "dummy_input = torch.randn(1, 4, 84, 80)\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941e68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones).astype(np.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "732461ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "buffer = ReplayBuffer(capacity=10)  # create a buffer\n",
    "print(len(buffer))                   # check how many items are in it (should be 0)\n",
    "\n",
    "# Add a dummy experience\n",
    "state = np.zeros((4, 84, 80))\n",
    "action = 0\n",
    "reward = 1.0\n",
    "next_state = np.ones((4, 84, 80))\n",
    "done = False\n",
    "buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(len(buffer))  # now it should be 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4efdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Epsilon-greedy Action Selection\n",
    "def epsilon_greedy(state, model, epsilon, n_actions):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        return model(state_tensor).max(1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07598fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training Loop\n",
    "def train_dqn(env, policy_net, target_net, optimizer, buffer, image_shape,\n",
    "              num_episodes=500, batch_size=8, target_update=10,\n",
    "              gamma=0.95, eps_start=1.0, eps_end=0.05, eps_decay=0.995):\n",
    "    epsilon = eps_start\n",
    "    n_actions = env.action_space.n\n",
    "    rewards_per_ep = []\n",
    "    avg_rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = get_initial_state(env, image_shape)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy(state, policy_net, epsilon, n_actions)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            processed_next = preprocess_frame(obs, image_shape).squeeze(0)\n",
    "            next_state = np.append(state[1:], [processed_next], axis=0)\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "                states_v = torch.tensor(states, dtype=torch.float32)\n",
    "                next_states_v = torch.tensor(next_states, dtype=torch.float32)\n",
    "                actions_v = torch.tensor(actions).unsqueeze(1)\n",
    "                rewards_v = torch.tensor(rewards).unsqueeze(1)\n",
    "                dones_v = torch.tensor(dones).unsqueeze(1)\n",
    "\n",
    "                q_vals = policy_net(states_v).gather(1, actions_v)\n",
    "                next_q_vals = target_net(next_states_v).max(1)[0].unsqueeze(1).detach()\n",
    "                q_targets = rewards_v + gamma * next_q_vals * (1 - dones_v)\n",
    "                loss = nn.MSELoss()(q_vals, q_targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            epsilon = max(epsilon * eps_decay, eps_end)\n",
    "\n",
    "        if ep % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        rewards_per_ep.append(total_reward)\n",
    "        avg_5 = np.mean(rewards_per_ep[-5:])\n",
    "        avg_rewards.append(avg_5)\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep} | Reward: {total_reward} | Avg5: {avg_5:.2f} | Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return rewards_per_ep, avg_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a9c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running config: batch_size=8, target_update=10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [8, 4, 84, 80, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     plt.show()\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Uncomment to run all experiments   \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m optimizer = optim.Adam(policy_net.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     19\u001b[39m buffer = ReplayBuffer()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m rewards, avg_rewards = \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_update\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_update\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m all_results.append((config, rewards, avg_rewards))\n\u001b[32m     28\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m,\u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_dqn\u001b[39m\u001b[34m(env, policy_net, target_net, optimizer, buffer, image_shape, num_episodes, batch_size, target_update, gamma, eps_start, eps_end, eps_decay)\u001b[39m\n\u001b[32m     30\u001b[39m rewards_v = torch.tensor(rewards).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     31\u001b[39m dones_v = torch.tensor(dones).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m q_vals = \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_v\u001b[49m\u001b[43m)\u001b[49m.gather(\u001b[32m1\u001b[39m, actions_v)\n\u001b[32m     34\u001b[39m next_q_vals = target_net(next_states_v).max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m].unsqueeze(\u001b[32m1\u001b[39m).detach()\n\u001b[32m     35\u001b[39m q_targets = rewards_v + gamma * next_q_vals * (\u001b[32m1\u001b[39m - dones_v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mDQNCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\habha\\Desktop\\Reinforcement Learning\\Assignment 3\\RL_assignment3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [8, 4, 84, 80, 1]"
     ]
    }
   ],
   "source": [
    "# 8. Run Experiments and Plot Results\n",
    "def run_experiments():\n",
    "    configs = [\n",
    "        {\"batch_size\": 8, \"target_update\": 10},\n",
    "        {\"batch_size\": 16, \"target_update\": 10},\n",
    "        {\"batch_size\": 8, \"target_update\": 3},\n",
    "        {\"batch_size\": 16, \"target_update\": 3},\n",
    "    ]\n",
    "    episodes = 500\n",
    "    all_results = []\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\"Running config: batch_size={config['batch_size']}, target_update={config['target_update']}\")\n",
    "        policy_net = DQNCNN(4, env.action_space.n)\n",
    "        target_net = DQNCNN(4, env.action_space.n)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "        buffer = ReplayBuffer()\n",
    "\n",
    "        rewards, avg_rewards = train_dqn(\n",
    "            env, policy_net, target_net, optimizer, buffer, image_shape,\n",
    "            num_episodes=episodes, batch_size=config['batch_size'], target_update=config['target_update']\n",
    "        )\n",
    "\n",
    "        all_results.append((config, rewards, avg_rewards))\n",
    "\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(rewards, label='Episode Reward')\n",
    "        plt.plot(avg_rewards, label='5-episode Average')\n",
    "        plt.title(f\"Batch Size: {config['batch_size']}, Target Update: {config['target_update']}\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Combined Plot\n",
    "    plt.figure(figsize=(14,8))\n",
    "    for config, _, avg_rewards in all_results:\n",
    "        label = f\"B{config['batch_size']}_U{config['target_update']}\"\n",
    "        plt.plot(avg_rewards, label=label)\n",
    "    plt.title(\"5-Episode Moving Average Reward for all Experiments\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to run all experiments   \n",
    "run_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
